{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setting Up TensorFlow\n",
    "By: Dylan Slack\n",
    "\n",
    "The goal of this notebook is to provide an easy to use guide to learn some of the basics of TensorFlow (TF).  It should correspond to slides posted along with this repo.  Follow along and learn! :) \n",
    "\n",
    "\n",
    "\n",
    "## Installing TensorFlow\n",
    "## (TODO: Before launching this notebook)\n",
    "Installing TF can be a trying process in its own right (ask Emile) — particularly when trying to get it to run on the GPU.  These instructions should work for the lab machines without setting up for GPU use.  If you're trying to install on you're own computer hopefully they work as well.\n",
    "\n",
    "---\n",
    "##### GPU Aside\n",
    "If you're wondering why we might want the GPU, GPU's allow us to perform lots of computations concurrently over many simple cores.  CPU's have a few complex cores.  If our computations are simple enough (which they are in many cases in deep learning, think matrix multiplication), we can let a GPU perform them in parallel.  This saves us *a lot* of time when we're building large models.\n",
    "\n",
    "---\n",
    "\n",
    "We're going to install TF in a virtual environment.  Virtual environments allow us to create different sets of dependencies for different projects; the good news is that if we screw something up trying to install TF in our virtual environment, it shouldn't mess anything else up!\n",
    "\n",
    "First, setup the virtual environment:\n",
    "\n",
    "``` conda create -n tf_tutorial ```\n",
    "\n",
    "You should be in the same directory as the virtual environment. Now:\n",
    "\n",
    "``` source activate tf_tutorial ```\n",
    "\n",
    "Install pip, TF, and another package that will help us manage different jupyter notebook kernels.  This could take a second:\n",
    "\n",
    "``` conda install pip```\n",
    "\n",
    "``` pip install tensorflow ```\n",
    "\n",
    "``` pip install matplotlib ```\n",
    "\n",
    "``` pip install pandas ```\n",
    "\n",
    "``` pip install sklearn ```\n",
    "\n",
    "```conda install nb_conda```\n",
    "\n",
    "Finally, load the notebook:\n",
    "\n",
    "```jupyter notebook```\n",
    "\n",
    "Navigate to this notebook and load it up!  Hopefully it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: TensorFlow Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Slide 7\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple graph example \n",
    "x = 4\n",
    "y = 2\n",
    "add = tf.add(x,y)\n",
    "mul = tf.multiply(x,y)\n",
    "output_1 = tf.multiply(add,mul)\n",
    "output_2 = tf.pow(add, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## What happens if we just print the tensor\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "## But, what if we include a session\n",
    "with tf.Session() as sess:\n",
    "    correct_output = sess.run(output_1)\n",
    "    print (correct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# Further, we can also evaluate output_2\n",
    "with tf.Session() as sess:\n",
    "    second_output = sess.run(output_2)\n",
    "    print (second_output)\n",
    "    \n",
    "# Starting to get the idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One: 48 Two: 36\n"
     ]
    }
   ],
   "source": [
    "# Also, this works\n",
    "with tf.Session() as sess:\n",
    "    one,two = sess.run([output_1, output_2])\n",
    "    print (\"One:\",one,\"Two:\",two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Slide 8\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0]\n",
      " [0 4]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " * There are many such operations we can do, here we multiple [3,4] by the indentity matrix.\n",
    " * We introduce constants, which we can name\n",
    " * We want to name them because we can visualize them using tensorboard -- a **really** useful graph visualization\n",
    "   tool\n",
    "\"\"\"\n",
    "\n",
    "m_1 = tf.constant([3,4], name=\"hello\")\n",
    "m_2 = tf.constant([[1,0],[0,1]], name=\"tensorflow\")\n",
    "r = tf.multiply(m_1,m_2, name=\"multiplication\")\n",
    "\n",
    "# We make our tensorboard call here. Run http://localhost:6006/#graphs&run=. to see the visualizaton\n",
    "# Run tensorboard --logdir=graphs from the home directory of this project\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(r))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " * Constants are bad because they're hardcoded into the defintion of the graph\n",
    " * Let's see what that means\n",
    "\"\"\"\n",
    "arr = [2.0,3.0]\n",
    "\n",
    "bad_constant = tf.constant(arr)\n",
    "with tf.Session() as sess:\n",
    "        # Uncover this print statement to see \n",
    "        # print (sess.graph.as_graph_def())\n",
    "        pass\n",
    "    \n",
    "\n",
    "# This starts to get out of hand for really large constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "# Slide 9\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.2048893 2.6847842 2.7117345]\n",
      " [2.0592587 1.1133807 1.8628153]\n",
      " [2.4075432 2.2154744 2.8005471]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " * Variables maintain the state of the graph across calls to run\n",
    " * Unlike constants they must be initialized\n",
    " \n",
    "NOTE: if you try and run this cell again, it will fail because there will be a     \n",
    "      variable that already exists with the same name.  Hit the >> button on \n",
    "      the toolbar uptop to resolve the issue!!\n",
    "\"\"\"\n",
    "\n",
    "# This still suffers from the variable loading problem :O\n",
    "bad_var_1 = tf.Variable(2, name=\"scalar_example\")\n",
    "\n",
    "# This is a better way to do things\n",
    "var_1 = tf.get_variable(\"scalar_example\", initializer=tf.constant(2.0)) \n",
    "var_2 = tf.get_variable(\"array_example\", initializer=tf.constant([1.0,0.0]))\n",
    "\n",
    "# This just gives a 3x3 matrix with random pulls from a normal distribution\n",
    "var_3 = tf.get_variable(\"other_array\", (3,3),\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "\n",
    "out_1 = tf.add(var_1, var_3)\n",
    "out_2 = tf.multiply(var_1, var_2)\n",
    "\n",
    "# We need to initialize these variables and do so as such\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print (sess.run(out_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "8.0\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "# Here's why they're variables\n",
    "\n",
    "the_output = var_1.assign(2.0 * var_1)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(var_1.initializer)\n",
    "    print (sess.run(the_output))\n",
    "    print (sess.run(the_output))\n",
    "    print (sess.run(the_output))\n",
    "    \n",
    "# var_1 retains it's value across session runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Slide 10\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Variables can have different values across different sessions!\n",
    "\"\"\"\n",
    "\n",
    "var_1 = tf.get_variable(\"sessions_example\", initializer=tf.constant(5))\n",
    "\n",
    "session_1 = tf.Session()\n",
    "session_2 = tf.Session()\n",
    "\n",
    "session_1.run(var_1.initializer)\n",
    "session_2.run(var_1.initializer)\n",
    "\n",
    "option_1 = var_1.assign(2 * var_1)\n",
    "other_option = var_1.assign(100 * var_1)\n",
    "print (session_1.run(option_1))\n",
    "print (session_2.run(other_option))\n",
    "\n",
    "# We need to close both of these sessions because we didn't use \"with\" here\n",
    "# With automatically closes the session once the program leaves the scope of with\n",
    "session_1.close() \n",
    "session_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "()\n",
      "---\n",
      "[3, 3]\n",
      "(2,)\n",
      "---\n",
      "[[1, 2], [3, 4]]\n",
      "(2, 2)\n",
      "---\n",
      "(5, 5, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Aside on shape.  If you're familiar with matrix operations with packages like numpy, skip this\n",
    "* It might be the case that readers don't have a good sense of how computations using matrices\n",
    "  are handled by matrix computation packages like TF but also popular packages like numpy and\n",
    "  pandas\n",
    "* Here we discuss this briefly\n",
    "\"\"\"\n",
    "\n",
    "# Suppose we have the scalar value \n",
    "a = 3 \n",
    "\n",
    "# We use a package called numpy that handles matrices really well to convert it to a matrix\n",
    "a_arr = np.array(3)\n",
    "print (a)\n",
    "print (a_arr.shape)\n",
    "print (\"---\")\n",
    "\n",
    "# We see that it has no shape. Consider next:\n",
    "b = [3,3]\n",
    "b_arr = np.array(b)\n",
    "print (b)\n",
    "print (b_arr.shape)\n",
    "print ('---')\n",
    "\n",
    "# The shape here is (2,) to reflect that there is one dimension with two values. Further:\n",
    "c = [[1,2],[3,4]]\n",
    "c_arr = np.array(c)\n",
    "print (c)\n",
    "print (c_arr.shape)\n",
    "print ('---')\n",
    "\n",
    "# Now the shape is (2,2) to reflect 2 values across two dimensions\n",
    "# We can create larger arrays with more dimensions\n",
    "\n",
    "d = np.random.rand(5,5,5,5)\n",
    "print (d.shape)\n",
    "\n",
    "# This array has four dimensions with 5 values of random numbers on the range of 0 - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Slide 11\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* We now introduce placeholders\n",
    "* Placeholders define what we want out data to look like.  We can include placeholders in \n",
    "  our graph in locations where we want to feed in data later on.\n",
    "\"\"\"\n",
    "\n",
    "# The shape of our expected input is [None, 3], meaning we accept any value in the dimension with None\n",
    "# and expect there to be three values in the second dimension\n",
    "in_arr = tf.placeholder(tf.float32, shape=[None,3])\n",
    "\n",
    "multiplier = tf.constant([1,2,3], tf.float32)\n",
    "\n",
    "out = tf.multiply(in_arr, multiplier)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # We include our desired input as a \"feed dict\"\n",
    "    print (sess.run(out, feed_dict={in_arr: [[1,0,0], [0,1,0], [0,0,1]]}))\n",
    "    \n",
    "# We compute the array [1,2,3] times the identity matrix in this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0\n"
     ]
    }
   ],
   "source": [
    "# Here's another example\n",
    "\n",
    "in_one = tf.placeholder(tf.float32, shape=3)\n",
    "in_two = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "out = tf.multiply(tf.reduce_sum(in_one),tf.add(in_two, in_two))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(out, feed_dict={in_one:[2,2,2], in_two:5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aside: a really bad coding practice in TF, don't do this\n",
    "\n",
    "You have to remember that tensorflow builds graph edges on calls like tf.add, tf.subtract, tf.multiply...\n",
    "If you loop over these calls, you'll just end up adding more edges to the graph\n",
    "\"\"\"\n",
    "\n",
    "x = tf.Variable(1, name=\"bad_one\")\n",
    "y = tf.Variable(2, name=\"bad_two\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5):\n",
    "        sess.run(tf.add(x,y)) # Could you save lines? Nope!\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the user cearly wants one operation for add.  However, a new operation is added on each call to tf.add\n",
    "so there's an operation added on every iteration. This is bad because the size of the graph could blow up in \n",
    "your face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building a Linear Regression in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Slide 13 Onward\n",
    "##\n",
    "from IPython.display import Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the data\n",
    "First, we're going to load the data.  I've already created a train-test split for the boston housing dataset.  The boston housing dataset provides housing values in the suburbs of boston in relation to features like crime, average rooms, age, tax, etc. All we have to do is load the data using pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./data/train_boston_housing.csv\"\n",
    "test_data_path = \"./data/test_boston_housing.csv\"\n",
    "\n",
    "# This is the label of the target variable\n",
    "# Its the median value of owner-occupied homes in $1,000s\n",
    "LABEL_COLUMN = \"medv\"\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "\n",
    "\"\"\"\n",
    "If you're interested, you can uncover the print statement to display the\n",
    "training data\n",
    "\"\"\"\n",
    "# print (train_data)\n",
    "\n",
    "train_data_y = train_data[LABEL_COLUMN].values\n",
    "train_data_y = train_data_y.reshape(train_data_y.shape[0],1)\n",
    "train_data_X = train_data.drop([LABEL_COLUMN], axis=1).values\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "test_data_y = test_data[LABEL_COLUMN].values\n",
    "test_data_y = test_data_y.reshape(test_data_y.shape[0],1)\n",
    "test_data_X = test_data.drop([LABEL_COLUMN], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess the data\n",
    "Next, we standardize features by removing the mean and scaling to unit variance.  Specifically, we compute z = (x - u)/s where x is the training data, u is the mean of the training data, s is the standard deviation of the training data, and z is the preprocessed training data.\n",
    "\n",
    "This is important to do because differences in the feature ranges can cause issues with fitting our model.  Making our data look like a gaussian with mean 0 and unit variance makes things work better.\n",
    "\n",
    "Of importance, we apply the same u and s that we do to the training data to the testing data.  If we don't, we're \"looking ahead\" to our training data which is a no-no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scal = StandardScaler()\n",
    "scal.fit(train_data_X)\n",
    "\n",
    "train_data_X = scal.transform(train_data_X)\n",
    "test_data_X = scal.transform(test_data_X)\n",
    "NUM_SAMPLES = train_data_X.shape[0]\n",
    "NUM_FEATURES = train_data_X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Setup the linear regression graph and optimizer\n",
    "I give the code first, then work through an explanation.  There's not a lot of code but a good amount of theory here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1)\n",
    "X = tf.placeholder(tf.float32,shape=[None,NUM_FEATURES])\n",
    "y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "# (2)\n",
    "weights = tf.get_variable('weights',shape=[NUM_FEATURES, 1],\n",
    "                          initializer=tf.random_normal_initializer())\n",
    "bias = tf.get_variable('bias',shape=[1],\n",
    "                       initializer=tf.random_normal_initializer())\n",
    "\n",
    "# (3)\n",
    "mat_out = tf.matmul(X, weights)\n",
    "y_pred = tf.add(mat_out, bias)\n",
    "\n",
    "# (4)\n",
    "loss = tf.reduce_sum(tf.square(y - y_pred)) / 2\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).\\\n",
    "    minimize(loss) #<< SO easy!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) We're now ready to setup the graph of our linear regression.  We let our regression accept inputs with the dimensions [None, NUM_FEATURES] because we want to allow for computation across multiple inputs.  The same rationale holds true for y.\n",
    "\n",
    "(2) We next initialize the weights and a bias using our variable initialization patterns from before.  These are our trainable parameters that we're going to optimize.\n",
    "\n",
    "(3) Finally, we can setup the computation for the outputs of our regression.  We define the output of our regression as follows: $y_{pred} = X \\cdot \\theta + b$\n",
    "\n",
    "Here, $\\theta$ represents our learned weights and $b$ is a bias term.  \n",
    "\n",
    "Let's think about the dimensionality for a second and why this works.  Consider if we had 20 training points X each with 10 features.  The dimensionality of X would be (20,10) representing 20 rows of training points stacked on top of one another each with 10 features.    \n",
    "\n",
    "$\\theta$ would have 10 weights (one for each feature).  By performing a matrix multiplication with dimensions $(20,10)\\cdot (10,1)$ we can get an output of $(20,1)$, which is the weights applied to each row.\n",
    "\n",
    "(4) We can now define our loss. The loss is a function we setup that defines our desirable our predictions are.  The loss we are using here is $$ \\mathcal{L_\\theta} = \\frac{(y^{(i)} - y_{pred}^{(i)})^2}{2}  $$  \n",
    "\n",
    "For simplicity, we only consider the loss over two inputs in this implementation.  However, a more generalized version of the loss would look like: $$ \\mathcal{L}_\\theta = \\frac{1}{2n} \\sum\\limits_{i=1}^{n}(y^{(i)} - y_{pred}^{(i)})^2$$\n",
    "\n",
    "Lastly, we can set up an optimizer called the GradientDescentOptimizer to compute the gradient descent algorithm over our variables. Gradient descent iteratively computes: \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\theta_{i+1} & = \\theta_{i} - \\alpha \\nabla_\\theta \\mathcal{L}_\\theta \\\\\n",
    " & =  \\theta_{i} - \\alpha(y^{(i)} - y_{pred}^{(i)})x^{(i)} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "The intuition for why this works is that we take our weights and compute the gradient with respect to a loss functions that defines a goal.  The goal here is to get the predictions as close to the actual as possible.  We do so by taking the gradient of our loss with respect to the current weights and then take a small \"step\" in the right direction.  The right direction is the opposite of the gradient because we want to minimize (thus subtraction).  We have $\\alpha$ as a multiplier to limit how far of a step we take.  We refer $\\alpha$ as the learning rate.  Smaller learning rates mean smaller steps.\n",
    "\n",
    "It's amazing that we can do all this with such a simple call in TensorFlow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The Learning Loop\n",
    "We're finally ready to setup the learning loop.  The learning loop iteratively performs updates on $\\theta$ using gradient descent.  We call our method by calling sess.run() on the optimizer over multiple passes through all the training data.   We really only call sess.run() on the loss so we can gt it for a print out. \n",
    "\n",
    "After we're done, we perform predictions on the test data set to evaluate how well we did.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Mean Loss: 203.7207158579685\n",
      "Epoch: 1 Mean Loss: 91.86812633962029\n",
      "Epoch: 2 Mean Loss: 46.76892905055296\n",
      "Epoch: 3 Mean Loss: 27.040234817608507\n",
      "Epoch: 4 Mean Loss: 18.31549652841273\n",
      "Epoch: 5 Mean Loss: 14.422666630707573\n",
      "Epoch: 6 Mean Loss: 12.662005137636116\n",
      "Epoch: 7 Mean Loss: 11.846505802636722\n",
      "Epoch: 8 Mean Loss: 11.452822254092283\n",
      "Epoch: 9 Mean Loss: 11.249580900348645\n",
      "Epoch: 10 Mean Loss: 11.134139232822983\n",
      "Epoch: 11 Mean Loss: 11.060740884882158\n",
      "Epoch: 12 Mean Loss: 11.008747823848253\n",
      "Epoch: 13 Mean Loss: 10.968701045546828\n",
      "Epoch: 14 Mean Loss: 10.936096067328497\n",
      "Epoch: 15 Mean Loss: 10.908657537852639\n",
      "Learned weights:\n",
      " [[-0.6015169 ]\n",
      " [ 0.5940588 ]\n",
      " [ 0.06025974]\n",
      " [ 0.87217134]\n",
      " [-1.373715  ]\n",
      " [ 3.2051513 ]\n",
      " [ 0.04334285]\n",
      " [-2.126282  ]\n",
      " [ 1.4754201 ]\n",
      " [-1.0264769 ]\n",
      " [-1.9868598 ]\n",
      " [ 0.8629196 ]\n",
      " [-3.1517959 ]]\n",
      "Learned bias:\n",
      " [22.523546]\n",
      "Test MSE: 27.119338979078805\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS+1):\n",
    "        accum_loss = 0\n",
    "        \n",
    "        # Iterate over the train data and labels\n",
    "        for x_val, y_val in zip(train_data_X, train_data_y):\n",
    "            _, l  = sess.run([optimizer, loss], \n",
    "                            feed_dict={X: [x_val], y: [y_val]})\n",
    "            accum_loss += l\n",
    "            \n",
    "        print (\"Epoch:\",i,\"Mean Loss:\",accum_loss / NUM_SAMPLES)\n",
    "            \n",
    "    # Here are the weights and bias we learn\n",
    "    print(\"Learned weights:\\n\",sess.run(weights))\n",
    "    print(\"Learned bias:\\n\",sess.run(bias))\n",
    "    \n",
    "    # Show how well we did on our test set\n",
    "    predicted_values = sess.run(y_pred, feed_dict={X: test_data_X})\n",
    "    test_mse = sess.run(tf.reduce_mean(tf.square(predicted_values - test_data_y)))\n",
    "    print (\"Test MSE:\",test_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_tutorial] *",
   "language": "python",
   "name": "conda-env-tf_tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
