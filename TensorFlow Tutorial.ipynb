{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal of this notebook is to provide an easy to use guide to learn some of the basics of TensorFlow (TF).  It should correspond to slides posted along with this repo.  Follow along and learn! :) \n",
    "\n",
    "\n",
    "\n",
    "## Installing TensorFlow\n",
    "## (TODO: Before launching this notebook)\n",
    "Installing TF can be a trying process in its own right (ask Emile) — particularly when trying to get it to run on the GPU.  These instructions should work for the lab machines without setting up for GPU use.  If you're trying to install on you're own computer hopefully they work as well.\n",
    "\n",
    "---\n",
    "##### GPU Aside\n",
    "If you're wondering why we might want the GPU, GPU's allow us to perform lots of computations concurrently over many simple cores.  CPU's have a few complex cores.  If our computations are simple enough (which they are in many cases in deep learning, think matrix multiplication), we can let a GPU perform them in parallel.  This saves us *a lot* of time when we're building large models.\n",
    "\n",
    "---\n",
    "\n",
    "We're going to install TF in a virtual environment.  Virtual environments allow us to create different sets of dependencies for different projects; the good news is that if we screw something up trying to install TF in our virtual environment, it shouldn't mess anything else up!\n",
    "\n",
    "First, setup the virtual environment:\n",
    "\n",
    "``` conda create -n tf_tutorial ```\n",
    "\n",
    "You should be in the same directory as the virtual environment. Now:\n",
    "\n",
    "``` source activate tf_tutorial ```\n",
    "\n",
    "Install pip, TF, and another package that will help us manage different jupyter notebook kernels.  This could take a second:\n",
    "\n",
    "``` conda install pip```\n",
    "\n",
    "``` pip install tensorflow ```\n",
    "\n",
    "```conda install nb_conda```\n",
    "\n",
    "Finally, load the notebook:\n",
    "\n",
    "```jupyter notebook```\n",
    "\n",
    "Navigate to this notebook and load it up!  Hopefully it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Slide 7\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple graph example \n",
    "x = 4\n",
    "y = 2\n",
    "add = tf.add(x,y)\n",
    "mul = tf.multiply(x,y)\n",
    "output_1 = tf.multiply(add,mul)\n",
    "output_2 = tf.pow(add, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## What happens if we just print the tensor\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "## But, what if we include a session\n",
    "with tf.Session() as sess:\n",
    "    correct_output = sess.run(output_1)\n",
    "    print (correct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# Further, we can also evaluate output_2\n",
    "with tf.Session() as sess:\n",
    "    second_output = sess.run(output_2)\n",
    "    print (second_output)\n",
    "    \n",
    "# Starting to get the idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One: 48 Two: 36\n"
     ]
    }
   ],
   "source": [
    "# Also, this works\n",
    "with tf.Session() as sess:\n",
    "    one,two = sess.run([output_1, output_2])\n",
    "    print (\"One:\",one,\"Two:\",two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Slide 8\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0]\n",
      " [0 4]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " * There are many such operations we can do, here we multiple [3,4] by the indentity matrix.\n",
    " * We introduce constants, which we can name\n",
    " * We want to name them because we can visualize them using tensorboard -- a **really** useful graph visualization\n",
    "   tool\n",
    "\"\"\"\n",
    "\n",
    "m_1 = tf.constant([3,4], name=\"hello\")\n",
    "m_2 = tf.constant([[1,0],[0,1]], name=\"tensorflow\")\n",
    "r = tf.multiply(m_1,m_2, name=\"multiplication\")\n",
    "\n",
    "# We make our tensorboard call here. Run http://localhost:6006/#graphs&run=. to see the visualizaton\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(r))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " * Constants are bad because they're hardcoded into the defintion of the graph\n",
    " * Let's see what that means\n",
    "\"\"\"\n",
    "arr = [2.0,3.0]\n",
    "\n",
    "bad_constant = tf.constant(arr)\n",
    "with tf.Session() as sess:\n",
    "        # Uncover this print statement to see \n",
    "        # print (sess.graph.as_graph_def())\n",
    "        pass\n",
    "    \n",
    "\n",
    "# This starts to get out of hand for really large constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "# Slide 9\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.396724   0.45323527 2.99148   ]\n",
      " [2.3861384  1.6340809  3.371299  ]\n",
      " [2.3580835  1.5679553  3.4670875 ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " * Variables maintain the state of the graph across calls to run\n",
    " * Unlike constants they must be initialized\n",
    " \n",
    "NOTE: if you try and run this cell again, it will fail because there will be a     \n",
    "      variable that already exists with the same name.  Hit the >> button on \n",
    "      the toolbar uptop to resolve the issue!!\n",
    "\"\"\"\n",
    "\n",
    "# This still suffers from the variable loading problem :O\n",
    "bad_var_1 = tf.Variable(2, name=\"scalar_example\")\n",
    "\n",
    "# This is a better way to do things\n",
    "var_1 = tf.get_variable(\"scalar_example\", initializer=tf.constant(2.0)) \n",
    "var_2 = tf.get_variable(\"array_example\", initializer=tf.constant([1.0,0.0]))\n",
    "\n",
    "# This just gives a 3x3 matrix with random pulls from a normal distribution\n",
    "var_3 = tf.get_variable(\"other_array\", (3,3), initializer=tf.random_normal_initializer())\n",
    "\n",
    "out_1 = tf.add(var_1, var_3)\n",
    "out_2 = tf.multiply(var_1, var_2)\n",
    "\n",
    "# We need to initialize these variables and do so as such\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print (sess.run(out_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "8.0\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "# Here's why they're variables\n",
    "\n",
    "the_output = var_1.assign(2.0 * var_1)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(var_1.initializer)\n",
    "    print (sess.run(the_output))\n",
    "    print (sess.run(the_output))\n",
    "    print (sess.run(the_output))\n",
    "    \n",
    "# var_1 retains it's value across session runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Slide 10\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Variables can have different values across different sessions!\n",
    "\"\"\"\n",
    "\n",
    "var_1 = tf.get_variable(\"sessions_example\", initializer=tf.constant(5))\n",
    "\n",
    "session_1 = tf.Session()\n",
    "session_2 = tf.Session()\n",
    "\n",
    "session_1.run(var_1.initializer)\n",
    "session_2.run(var_1.initializer)\n",
    "\n",
    "option_1 = var_1.assign(2 * var_1)\n",
    "other_option = var_1.assign(100 * var_1)\n",
    "print (session_1.run(option_1))\n",
    "print (session_2.run(other_option))\n",
    "\n",
    "# We need to close both of these sessions because we didn't use \"with\" here\n",
    "# With automatically closes the session once the program leaves the scope of with\n",
    "session_1.close() \n",
    "session_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "()\n",
      "---\n",
      "[3, 3]\n",
      "(2,)\n",
      "---\n",
      "[[1, 2], [3, 4]]\n",
      "(2, 2)\n",
      "---\n",
      "(5, 5, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Aside on shape.  If you're familiar with matrix operations with packages like numpy, skip this\n",
    "* It might be the case that readers don't have a good sense of how computations using matrices\n",
    "  are handled by matrix computation packages like TF but also popular packages like numpy and\n",
    "  pandas\n",
    "* Here we discuss this briefly\n",
    "\"\"\"\n",
    "\n",
    "# Suppose we have the scalar value \n",
    "a = 3 \n",
    "\n",
    "# We use a package called numpy that handles matrices really well to convert it to a matrix\n",
    "a_arr = np.array(3)\n",
    "print (a)\n",
    "print (a_arr.shape)\n",
    "print (\"---\")\n",
    "\n",
    "# We see that it has no shape. Consider next:\n",
    "b = [3,3]\n",
    "b_arr = np.array(b)\n",
    "print (b)\n",
    "print (b_arr.shape)\n",
    "print ('---')\n",
    "\n",
    "# The shape here is (2,) to reflect that there is one dimension with two values. Further:\n",
    "c = [[1,2],[3,4]]\n",
    "c_arr = np.array(c)\n",
    "print (c)\n",
    "print (c_arr.shape)\n",
    "print ('---')\n",
    "\n",
    "# Now the shape is (2,2) to reflect 2 values across two dimensions\n",
    "# We can create larger arrays with more dimensions\n",
    "\n",
    "d = np.random.rand(5,5,5,5)\n",
    "print (d.shape)\n",
    "\n",
    "# This array has four dimensions with 5 values of random numbers on the range of 0 - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Slide 11\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* We now introduce placeholders\n",
    "* Placeholders define what we want out data to look like.  We can include placeholders in \n",
    "  our graph in locations where we want to feed in data later on.\n",
    "\"\"\"\n",
    "\n",
    "# The shape of our expected input is [None, 3], meaning we accept any value in the dimension with None\n",
    "# and expect there to be three values in the second dimension\n",
    "in_arr = tf.placeholder(tf.float32, shape=[None,3])\n",
    "\n",
    "multiplier = tf.constant([1,2,3], tf.float32)\n",
    "\n",
    "out = tf.multiply(in_arr, multiplier)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # We include our desired input as a \"feed dict\"\n",
    "    print (sess.run(out, feed_dict={in_arr: [[1,0,0], [0,1,0], [0,0,1]]}))\n",
    "    \n",
    "# We compute the array [1,2,3] times the identity matrix in this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.]\n"
     ]
    }
   ],
   "source": [
    "# Here's another example\n",
    "\n",
    "in_one = tf.placeholder(tf.float32, shape=3)\n",
    "in_two = tf.placeholder(tf.float32, shape=1)\n",
    "\n",
    "out = tf.multiply(tf.reduce_sum(in_one),tf.add(in_two, in_two))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(out, feed_dict={in_one:[2,2,2], in_two:[5]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aside: a really bad coding practice in TF, don't do this\n",
    "\n",
    "You have to remember that tensorflow builds graph edges on calls like tf.add, tf.subtract, tf.multiply...\n",
    "If you loop over these calls, you'll just end up adding more edges to the graph\n",
    "\"\"\"\n",
    "\n",
    "x = tf.Variable(1)\n",
    "y = tf.Variable(2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5):\n",
    "        sess.run(tf.add(x,y)) # Could you save lines? Nope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here, the user cearly wants one operation for add.  However, a new operation is added on each call to tf.add\n",
    "so there's an operation added on every iteration. This is bad because the size of the graph could blow up in \n",
    "your face.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_tutorial] *",
   "language": "python",
   "name": "conda-env-tf_tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
